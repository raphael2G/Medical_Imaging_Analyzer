{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Rrca4g8zANZj",
        "outputId": "84974a65-aa12-456d-beb6-83c003c63fc1"
      },
      "outputs": [],
      "source": [
        "#@title Install and Import Dependencies\n",
        "%pip install einops\n",
        "%pip install tensorflowjs\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "import tensorflowjs as tfjs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        },
        "id": "QO2WRQAZADwT",
        "outputId": "f44392d2-4d79-4268-dd91-e6f1ccbbc7df"
      },
      "outputs": [],
      "source": [
        "#@title ViT \n",
        "import tensorflow as tf\n",
        "from keras import Model\n",
        "from keras.layers import Layer\n",
        "from keras import Sequential\n",
        "import keras.layers as nn\n",
        "\n",
        "from tensorflow import einsum\n",
        "from einops import rearrange, repeat\n",
        "from einops.layers.tensorflow import Rearrange\n",
        "\n",
        "def pair(t):\n",
        "    return t if isinstance(t, tuple) else (t, t)\n",
        "\n",
        "class PreNorm(Layer):\n",
        "    def __init__(self, fn):\n",
        "        super(PreNorm, self).__init__()\n",
        "\n",
        "        self.norm = nn.LayerNormalization()\n",
        "        self.fn = fn\n",
        "\n",
        "    def call(self, x, training=True):\n",
        "        return self.fn(self.norm(x), training=training)\n",
        "\n",
        "class MLP(Layer):\n",
        "    def __init__(self, dim, hidden_dim, dropout=0.0):\n",
        "        super(MLP, self).__init__()\n",
        "\n",
        "        def GELU():\n",
        "            def gelu(x, approximate=False):\n",
        "                if approximate:\n",
        "                    coeff = tf.cast(0.044715, x.dtype)\n",
        "                    return 0.5 * x * (1.0 + tf.tanh(0.7978845608028654 * (x + coeff * tf.pow(x, 3))))\n",
        "                else:\n",
        "                    return 0.5 * x * (1.0 + tf.math.erf(x / tf.cast(1.4142135623730951, x.dtype)))\n",
        "\n",
        "            return nn.Activation(gelu)\n",
        "\n",
        "        self.net = Sequential([\n",
        "            nn.Dense(units=hidden_dim),\n",
        "            GELU(),\n",
        "            nn.Dropout(rate=dropout),\n",
        "            nn.Dense(units=dim),\n",
        "            nn.Dropout(rate=dropout)\n",
        "        ])\n",
        "\n",
        "    def call(self, x, training=True):\n",
        "        return self.net(x, training=training)\n",
        "\n",
        "class Attention(Layer):\n",
        "    def __init__(self, dim, heads=8, dim_head=64, dropout=0.0):\n",
        "        super(Attention, self).__init__()\n",
        "        inner_dim = dim_head * heads\n",
        "        project_out = not (heads == 1 and dim_head == dim)\n",
        "\n",
        "        self.heads = heads\n",
        "        self.scale = dim_head ** -0.5\n",
        "\n",
        "        self.attend = nn.Softmax()\n",
        "        self.to_qkv = nn.Dense(units=inner_dim * 3, use_bias=False)\n",
        "\n",
        "        if project_out:\n",
        "            self.to_out = [\n",
        "                nn.Dense(units=dim),\n",
        "                nn.Dropout(rate=dropout)\n",
        "            ]\n",
        "        else:\n",
        "            self.to_out = []\n",
        "\n",
        "        self.to_out = Sequential(self.to_out)\n",
        "\n",
        "    def call(self, x, training=True):\n",
        "        qkv = self.to_qkv(x)\n",
        "        qkv = tf.split(qkv, num_or_size_splits=3, axis=-1)\n",
        "        q, k, v = map(lambda t: rearrange(t, 'b n (h d) -> b h n d', h=self.heads), qkv)\n",
        "\n",
        "        # dots = tf.matmul(q, tf.transpose(k, perm=[0, 1, 3, 2])) * self.scale\n",
        "        dots = einsum('b h i d, b h j d -> b h i j', q, k) * self.scale\n",
        "        attn = self.attend(dots)\n",
        "\n",
        "        # x = tf.matmul(attn, v)\n",
        "        x = einsum('b h i j, b h j d -> b h i d', attn, v)\n",
        "        x = rearrange(x, 'b h n d -> b n (h d)')\n",
        "        x = self.to_out(x, training=training)\n",
        "\n",
        "        return x\n",
        "\n",
        "class Transformer(Layer):\n",
        "    def __init__(self, dim, depth, heads, dim_head, mlp_dim, dropout=0.0):\n",
        "        super(Transformer, self).__init__()\n",
        "\n",
        "        self.layers = []\n",
        "\n",
        "        for _ in range(depth):\n",
        "            self.layers.append([\n",
        "                PreNorm(Attention(dim, heads=heads, dim_head=dim_head, dropout=dropout)),\n",
        "                PreNorm(MLP(dim, mlp_dim, dropout=dropout))\n",
        "            ])\n",
        "\n",
        "    def call(self, x, training=True):\n",
        "        for attn, mlp in self.layers:\n",
        "            x = attn(x, training=training) + x\n",
        "            x = mlp(x, training=training) + x\n",
        "\n",
        "        return x\n",
        "\n",
        "class ViT(Model):\n",
        "    def __init__(self, image_size, patch_size, num_classes, dim, depth, heads, mlp_dim,\n",
        "                 pool='cls', dim_head=64, dropout=0.0, emb_dropout=0.0):\n",
        "        \"\"\"\n",
        "            image_size: int.\n",
        "            -> Image size. If you have rectangular images, make sure your image size is the maximum of the width and height\n",
        "            patch_size: int.\n",
        "            -> Number of patches. image_size must be divisible by patch_size.\n",
        "            -> The number of patches is: n = (image_size // patch_size) ** 2 and n must be greater than 16.\n",
        "            num_classes: int.\n",
        "            -> Number of classes to classify.\n",
        "            dim: int.\n",
        "            -> Last dimension of output tensor after linear transformation nn.Linear(..., dim).\n",
        "            depth: int.\n",
        "            -> Number of Transformer blocks.\n",
        "            heads: int.\n",
        "            -> Number of heads in Multi-head Attention layer.\n",
        "            mlp_dim: int.\n",
        "            -> Dimension of the MLP (FeedForward) layer.\n",
        "            dropout: float between [0, 1], default 0..\n",
        "            -> Dropout rate.\n",
        "            emb_dropout: float between [0, 1], default 0.\n",
        "            -> Embedding dropout rate.\n",
        "            pool: string, either cls token pooling or mean pooling\n",
        "        \"\"\"\n",
        "        super(ViT, self).__init__()\n",
        "\n",
        "        image_height, image_width = pair(image_size)\n",
        "        patch_height, patch_width = pair(patch_size)\n",
        "\n",
        "        assert image_height % patch_height == 0 and image_width % patch_width == 0, 'Image dimensions must be divisible by the patch size.'\n",
        "\n",
        "        num_patches = (image_height // patch_height) * (image_width // patch_width)\n",
        "        assert pool in {'cls', 'mean'}, 'pool type must be either cls (cls token) or mean (mean pooling)'\n",
        "\n",
        "        self.patch_embedding = Sequential([\n",
        "            Rearrange('b (h p1) (w p2) c -> b (h w) (p1 p2 c)', p1=patch_height, p2=patch_width),\n",
        "            nn.Dense(units=dim)\n",
        "        ], name='patch_embedding')\n",
        "\n",
        "        self.pos_embedding = tf.Variable(initial_value=tf.random.normal([1, num_patches + 1, dim]))\n",
        "        self.cls_token = tf.Variable(initial_value=tf.random.normal([1, 1, dim]))\n",
        "        self.dropout = nn.Dropout(rate=emb_dropout)\n",
        "\n",
        "        self.transformer = Transformer(dim, depth, heads, dim_head, mlp_dim, dropout)\n",
        "\n",
        "        self.pool = pool\n",
        "\n",
        "        self.mlp_head = Sequential([\n",
        "            nn.LayerNormalization(),\n",
        "            nn.Dense(units=num_classes)\n",
        "        ], name='mlp_head')\n",
        "\n",
        "    def call(self, img, training=True, **kwargs):\n",
        "        x = self.patch_embedding(img)\n",
        "        b, n, d = x.shape\n",
        "\n",
        "        cls_tokens = repeat(self.cls_token, '() n d -> b n d', b=b)\n",
        "        x = tf.concat([cls_tokens, x], axis=1)\n",
        "        x += self.pos_embedding[:, :(n + 1)]\n",
        "        x = self.dropout(x, training=training)\n",
        "\n",
        "        x = self.transformer(x, training=training)\n",
        "\n",
        "        if self.pool == 'mean':\n",
        "            x = tf.reduce_mean(x, axis=1)\n",
        "        else:\n",
        "            x = x[:, 0]\n",
        "\n",
        "        x = self.mlp_head(x)\n",
        "\n",
        "        return x\n",
        "\n",
        "\"\"\" Usage\n",
        "\n",
        "v = ViT(\n",
        "    image_size = 256,\n",
        "    patch_size = 32,\n",
        "    num_classes = 1000,\n",
        "    dim = 1024,\n",
        "    depth = 6,\n",
        "    heads = 16,\n",
        "    mlp_dim = 2048,\n",
        "    dropout = 0.1,\n",
        "    emb_dropout = 0.1\n",
        ")\n",
        "\n",
        "img = tf.random.normal(shape=[1, 256, 256, 3])\n",
        "preds = v(img) # (1, 1000)\n",
        "\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "upxat8Gb_6g4"
      },
      "outputs": [],
      "source": [
        "#@title Training Loop\n",
        "import tensorflow as tf\n",
        "import tensorflow_datasets as tfds\n",
        "import keras\n",
        "from keras import layers\n",
        "from matplotlib import pyplot as plt\n",
        "\n",
        "from datetime import datetime\n",
        "import os\n",
        "\n",
        "def plot_data(loss_history, accuracy_history, validation_accuracy_history, save_figs=False):\n",
        "    fig, axs = plt.subplots(2)\n",
        "    fig.suptitle('Training Data')\n",
        "\n",
        "    axs[0].plot(accuracy_history, label='training accuracy')\n",
        "    axs[0].plot(validation_accuracy_history, label='validation accuracy')\n",
        "    axs[0].legend()\n",
        "\n",
        "\n",
        "    axs[1].plot(loss_history, label='loss')\n",
        "    axs[1].legend()\n",
        "\n",
        "    if save_figs:\n",
        "      axs[0].save_fig('/content/drive/MyDrive/savedModels/CNNCovidClassifier/graphs/training_val_accuracy.png')\n",
        "      axs[1].save_fig('/content/drive/MyDrive/savedModels/CNNCovidClassifier/graphs/training_loss.png')\n",
        "\n",
        "\n",
        "loss_fn = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
        "\n",
        "def calc_loss(model, x, y):\n",
        "        y_ = model(x)\n",
        "        loss = loss_fn(y, y_)\n",
        "        return loss\n",
        "\n",
        "def calc_grad(model, x, y):\n",
        "    with tf.GradientTape() as tape:\n",
        "        loss = calc_loss(model, x, y)\n",
        "    return tape.gradient(loss, model.trainable_variables), loss\n",
        "\n",
        "def train_model(model, n_epochs, train_ds, batch_size, learning_rate=3e-5, with_plot=True, validation_ds=None, \n",
        "                update_increment=10, model_name=None, checkpoint_dir='savedModels', save_figs=False):\n",
        "    \n",
        "    ds_train_batch = train_ds.batch(batch_size)\n",
        "\n",
        "    try: \n",
        "        ds_test_batch = validation_ds.batch(len(validation_ds)-1)\n",
        "        print('Beggining Training with Validation')\n",
        "        with_validation = True\n",
        "    except: \n",
        "        with_validation = False\n",
        "        print('Beggining Training without Validation')\n",
        "\n",
        "\n",
        "    decay_steps = 1000\n",
        "    lr_decayed_fn = tf.keras.optimizers.schedules.CosineDecay(\n",
        "        learning_rate, decay_steps)\n",
        "    optimizer = tf.keras.optimizers.Adam(lr_decayed_fn)\n",
        "\n",
        "    loss_history, accuracy_history, validation_accuracy_history = [], [], []\n",
        "\n",
        "    start_time = datetime.now()\n",
        "    for epoch in range(n_epochs):\n",
        "        epoch_loss = tf.keras.metrics.Mean()\n",
        "        epoch_accuracy = tf.keras.metrics.SparseCategoricalAccuracy()\n",
        "        validation_accuracy = tf.keras.metrics.SparseCategoricalAccuracy()\n",
        "\n",
        "        for x, y in ds_train_batch:\n",
        "            gradient, loss = calc_grad(model, x, y)\n",
        "            optimizer.apply_gradients(zip(gradient, model.trainable_variables))\n",
        "\n",
        "            epoch_loss.update_state(loss)  \n",
        "            epoch_accuracy.update_state(y, model(x))\n",
        "\n",
        "        loss_history.append(epoch_loss.result())\n",
        "        accuracy_history.append(epoch_accuracy.result())\n",
        "   \n",
        "\n",
        "        if with_validation: \n",
        "            for x, y in ds_test_batch:\n",
        "                validation_accuracy.update_state(y, model(x))\n",
        "\n",
        "            val_acc = validation_accuracy.result()\n",
        "            validation_accuracy_history.append(val_acc)\n",
        "            validation_accuracy.reset_state()\n",
        "\n",
        "\n",
        "        if epoch % update_increment == 0:\n",
        "            elapsed = datetime.now() - start_time\n",
        "            print('Epoch: %i' %epoch, 'Validation Accuracy: %.4f' %val_acc, 'Training Accuracy: %.4f' %epoch_accuracy.result(), 'Loss: %.6f' %epoch_loss.result(), 'Time: ' + datetime.now().strftime(\"%H:%M:%S\"))\n",
        "            \n",
        "            start_time = datetime.now()\n",
        "\n",
        "\n",
        "        \n",
        "    if model_name!=None:\n",
        "        try:\n",
        "            model.save(os.path.join(checkpoint_dir, model_name))\n",
        "            print('Model Weights Saved')\n",
        "        except:\n",
        "            if os.path.exists(os.path.join(checkpoint_dir, model_name)):\n",
        "                print('Error: model not saved')\n",
        "            else: print('Model not saved. Ensure checkpoint_path is valid.')\n",
        "\n",
        "    if with_plot:\n",
        "        if with_validation: plot_data(loss_history, accuracy_history, validation_accuracy_history, save_figs)\n",
        "        else:               plot_data(loss_history, accuracy_history, save_figs)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "cellView": "form",
        "id": "gV06u5QgATR8"
      },
      "outputs": [],
      "source": [
        "#@title Create Model\n",
        "# model = tf.keras.models.Sequential()\n",
        "# model.add(layers.Conv2D(32, (3, 3), activation='relu', input_shape=(224, 224, 3)))\n",
        "# model.add(layers.MaxPooling2D((2, 2)))\n",
        "# model.add(layers.Conv2D(64, (3, 3), activation='relu'))\n",
        "# model.add(layers.MaxPooling2D((2, 2)))\n",
        "# model.add(layers.Conv2D(64, (3, 3), activation='relu'))\n",
        "# model.add(layers.MaxPooling2D((2, 2)))\n",
        "# model.add(layers.Conv2D(64, (3, 3), activation='relu'))\n",
        "# model.add(layers.Flatten())\n",
        "# model.add(layers.Dense(64, activation='relu'))\n",
        "# model.add(layers.Dense(2))\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "cellView": "form",
        "id": "RQoatqm0f1zO"
      },
      "outputs": [],
      "source": [
        "#@title Create ViT\n",
        "model = ViT(\n",
        "    image_size = 224,\n",
        "    patch_size = 32,\n",
        "    num_classes = 2,\n",
        "    dim = 1024,\n",
        "    depth = 6,\n",
        "    heads = 16,\n",
        "    mlp_dim = 2048,\n",
        "    dropout = 0.1,\n",
        "    emb_dropout = 0.1\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "cellView": "form",
        "id": "9nFKDplr1Wt8"
      },
      "outputs": [],
      "source": [
        "#@title create_file_list()\n",
        "def create_file_list(non_covid_file_name_path, covid_file_name_path, training_file, labels_file, from_drive=False, drive_start='/content/drive/MyDrive'):\n",
        "    non_covid_image_folder_path = 'COVID-CT/Images-processed/CT_NonCOVID'\n",
        "    covid_image_folder_path = 'COVID-CT/Images-processed/CT_COVID'\n",
        "    \n",
        "    if from_drive:\n",
        "      non_covid_file_name_path = os.path.join(drive_start, non_covid_file_name_path)\n",
        "      covid_file_name_path = os.path.join(drive_start, covid_file_name_path)\n",
        "      training_file = os.path.join(drive_start, training_file)\n",
        "      labels_file = os.path.join(drive_start, labels_file)\n",
        "      non_covid_image_folder_path = os.path.join(drive_start, non_covid_image_folder_path)\n",
        "      covid_image_folder_path = os.path.join(drive_start, covid_image_folder_path)\n",
        "\n",
        "    non_covid_file_name_list = open(non_covid_file_name_path).readlines()\n",
        "    covid_file_name_list = open(covid_file_name_path).readlines()\n",
        "\n",
        "    f = open(training_file, 'w')\n",
        "    g = open(labels_file, 'w')\n",
        "\n",
        "    for non_covid_file_name, covid_file_name in zip(non_covid_file_name_list, covid_file_name_list):\n",
        "        \n",
        "        non_covid_file_name = non_covid_file_name.strip()\n",
        "        covid_file_name = covid_file_name.strip()\n",
        "\n",
        "        non_covid_path = os.path.join(non_covid_image_folder_path, non_covid_file_name)\n",
        "        covid_path = os.path.join(covid_image_folder_path, covid_file_name)\n",
        "        f.write(non_covid_path + '\\n')\n",
        "        g.write('0\\n')\n",
        "        f.write(covid_path + '\\n')\n",
        "        g.write('1\\n')\n",
        "\n",
        "    f.close()\n",
        "    g.close()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "cellView": "form",
        "id": "VjluEQeT1lFF"
      },
      "outputs": [],
      "source": [
        "#@title create_data_from_list()\n",
        "def create_data_from_list(data_folder, from_drive=False, drive_start='/content/drive/MyDrive'):\n",
        "    if from_drive:\n",
        "      data_folder = os.path.join(drive_start, data_folder)\n",
        "      \n",
        "    data_list = os.path.join(data_folder, 'data.txt')\n",
        "    label_list = os.path.join(data_folder, 'labels.txt')\n",
        "\n",
        "    training_data_list = open(data_list).readlines()\n",
        "    training_label_list = open(label_list).readlines()\n",
        "\n",
        "    training_tensors_data = []\n",
        "    for i, file in enumerate(training_data_list):\n",
        "        training_data_list[i] = file.strip()\n",
        "        training_tensors_data.append(process_path(file.strip()))\n",
        "\n",
        "    for i, label in enumerate(training_label_list):\n",
        "        training_label_list[i] = int(label.strip())\n",
        "    tensor_labels = tf.convert_to_tensor(training_label_list)\n",
        "\n",
        "    IMG_SIZE = 224\n",
        "    data_augmentation = tf.keras.Sequential([\n",
        "        layers.Resizing(IMG_SIZE, IMG_SIZE, 'bilinear'),\n",
        "        layers.Rescaling(1./255),\n",
        "        layers.RandomFlip(\"horizontal_and_vertical\"),\n",
        "    ])\n",
        "\n",
        "    for i, tensor_data in enumerate(training_tensors_data):\n",
        "        training_tensors_data[i] = data_augmentation(tensor_data)\n",
        "\n",
        "    training_tensors_data = tf.convert_to_tensor(training_tensors_data)\n",
        "    dataset = tf.data.Dataset.from_tensor_slices((training_tensors_data, tensor_labels))\n",
        "    \n",
        "    return dataset\n",
        "\n",
        "def process_path(file_path):\n",
        "  img = Image.open(file_path)\n",
        "  if np.shape(img)[-1] != 3:\n",
        "      try: \n",
        "          img = img.convert('RGB')\n",
        "      except:\n",
        "          print('Invalid Data Removed: ')\n",
        "          print(np.shape(img))\n",
        "          return '', False\n",
        "\n",
        "  img = np.asarray(img)\n",
        "  return tf.convert_to_tensor(img)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "cellView": "form",
        "id": "mFq7vVwTJp4d"
      },
      "outputs": [],
      "source": [
        "#@title Create data.txt and labels.txt\n",
        "non_covid_training_file_name_path = 'COVID-CT/Data-split/NonCOVID/trainCT_NonCOVID.txt'\n",
        "covid_training_file_name_path = 'COVID-CT/Data-split/COVID/trainCT_COVID.txt'\n",
        "training_data_path = 'COVID-CT/Data-split/Training/data.txt'\n",
        "training_label_path = 'COVID-CT/Data-split/Training/labels.txt'\n",
        "\n",
        "non_covid_validation_file_name_path = 'COVID-CT/Data-split/NonCOVID/valCT_NonCOVID.txt'\n",
        "covid_validation_file_name_path = 'COVID-CT/Data-split/COVID/valCT_COVID.txt'\n",
        "validation_data_path = 'COVID-CT/Data-split/Validation/data.txt'\n",
        "validation_label_path = 'COVID-CT/Data-split/Validation/labels.txt'\n",
        "\n",
        "train_ds = create_file_list(non_covid_training_file_name_path, covid_training_file_name_path, training_data_path, training_label_path, from_drive=True)\n",
        "validation_ds = create_file_list(non_covid_validation_file_name_path, covid_validation_file_name_path, validation_data_path, validation_label_path, from_drive=True)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "cellView": "form",
        "id": "GfmqRPZ2DTp1"
      },
      "outputs": [],
      "source": [
        "#@title Create dataset from data.txt and labels.txt\n",
        "training_info_folder = 'COVID-CT/Data-split/Training'\n",
        "validation_info_folder = 'COVID-CT/Data-split/Validation'\n",
        "train_ds = create_data_from_list(training_info_folder, from_drive=True)\n",
        "validation_ds = create_data_from_list(validation_info_folder, from_drive=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 389
        },
        "id": "o-Zvt-UwDMP4",
        "outputId": "3271580c-647f-4edc-89c7-c541b695a48b"
      },
      "outputs": [],
      "source": [
        "train_model(model, n_epochs=50, train_ds=train_ds, validation_ds=validation_ds, learning_rate= 0.0001, batch_size=16, update_increment=10, save_figs=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lJ_7vdlVCa3G",
        "outputId": "fb401923-35af-45e6-88c2-80b1e73f561f"
      },
      "outputs": [],
      "source": [
        "model.save('/content/drive/MyDrive/savedModels/CNNCovidClassifier/tfModel')\n",
        "tfjs.converters.save_keras_model(model, '/content/drive/MyDrive/savedModels/CNNCovidClassifier/tfjsModel')\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "TPU",
    "colab": {
      "collapsed_sections": [],
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
